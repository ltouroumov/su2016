{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.ensemble.forest import RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.ensemble.gradient_boosting import GradientBoostingRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.tree.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model.ridge import Ridge\n",
    "import sklearn.metrics\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors.regression import KNeighborsRegressor\n",
    "from sklearn import cross_validation\n",
    "from sklearn import datasets\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from pprint import pprint\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_dataset(file, columns, value_filter=lambda x: x):\n",
    "    with open(file, \"r\") as fd:\n",
    "        csv_reader = csv.DictReader(fd, fieldnames=columns, delimiter=\";\")\n",
    "        _header = next(csv_reader)\n",
    "        print(\"Loading dataset ...\")\n",
    "        return [{key: value_filter(value) for key, value in row.items()} for row in csv_reader]\n",
    "\n",
    "\n",
    "def filter_dataset(dataset, features, transform=lambda x: x):\n",
    "    return [[transform(row[feature]) for feature in features] for row in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset ...\n"
     ]
    }
   ],
   "source": [
    "def cleanup_data(value):\n",
    "    return value if value not in [\"NR\", \"\"] else -1\n",
    "\n",
    "\n",
    "# Open the file\n",
    "train_data_file = \"./data/ech_apprentissage.csv\"\n",
    "columns = [\n",
    "    'id',\n",
    "    'annee_naissance',\n",
    "    'annee_permis',\n",
    "    'marque',\n",
    "    'puis_fiscale',\n",
    "    'anc_veh',\n",
    "    'codepostal',\n",
    "    'energie_veh',\n",
    "    'kmage_annuel',\n",
    "    'crm',\n",
    "    'profession',\n",
    "    'var1',\n",
    "    'var2',\n",
    "    'var3',\n",
    "    'var4',\n",
    "    'var5',\n",
    "    'var6',\n",
    "    'var7',\n",
    "    'var8',\n",
    "    'var9',\n",
    "    'var10',\n",
    "    'var11',\n",
    "    'var12',\n",
    "    'var13',\n",
    "    'var14',\n",
    "    'var15',\n",
    "    'var16',\n",
    "    'var17',\n",
    "    'var18',\n",
    "    'var19',\n",
    "    'var20',\n",
    "    'var21',\n",
    "    'var22',\n",
    "    'prime_tot_ttc'\n",
    "]\n",
    "dataset = load_dataset(file=train_data_file,\n",
    "                       columns=columns,\n",
    "                       value_filter=cleanup_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering features ...\n",
      "feature_to_filter = ['annee_naissance', 'annee_permis', 'puis_fiscale', 'anc_veh', 'kmage_annuel', 'crm', 'var3', 'var4', 'var5', 'var7', 'var9', 'var11', 'var12', 'var13', 'var15', 'var17', 'var18', 'var19', 'var20', 'var21', 'var22']\n",
      "Splitting dataset\n",
      "train_dataset_size = 210000\n",
      "test_dataset_size = 90000\n"
     ]
    }
   ],
   "source": [
    "print(\"Filtering features ...\")\n",
    "# [\"crm\", \"annee_naissance\", \"kmage_annuel\"]\n",
    "feature_to_filter = [\n",
    "    # 'id',\n",
    "    'annee_naissance',\n",
    "    'annee_permis',\n",
    "    # 'marque',\n",
    "    'puis_fiscale',\n",
    "    'anc_veh',\n",
    "    # 'codepostal',\n",
    "    # 'energie_veh',\n",
    "    'kmage_annuel',\n",
    "    'crm',\n",
    "    # 'profession',\n",
    "    # 'var1',\n",
    "    # 'var2',\n",
    "    'var3',\n",
    "    'var4',\n",
    "    'var5',\n",
    "    # 'var6',\n",
    "    'var7',\n",
    "    # 'var8',\n",
    "    'var9',\n",
    "    # 'var10',\n",
    "    'var11',\n",
    "    'var12',\n",
    "    'var13',\n",
    "    # 'var14',\n",
    "    'var15',\n",
    "    # 'var16',\n",
    "    'var17',\n",
    "    'var18',\n",
    "    'var19',\n",
    "    'var20',\n",
    "    'var21',\n",
    "    'var22',\n",
    "    # 'prime_tot_ttc'\n",
    "]\n",
    "#feature_to_filter = [\n",
    "#    feature\n",
    "#    for feature in columns\n",
    "#    if feature not in feature_list\n",
    "#]\n",
    "print(\"feature_to_filter = {}\".format(feature_to_filter))\n",
    "dataset_filtered = filter_dataset(dataset=dataset,\n",
    "                                  features=feature_to_filter,\n",
    "                                  transform=float)\n",
    "\n",
    "targets = [float(row[\"prime_tot_ttc\"]) for row in dataset]\n",
    "print(\"Splitting dataset\")\n",
    "\n",
    "dataset_size = len(dataset_filtered)\n",
    "train_dataset_size = int(dataset_size * 0.7)\n",
    "test_dataset_size = dataset_size - train_dataset_size\n",
    "\n",
    "train_dataset = dataset_filtered[:train_dataset_size]\n",
    "test_dataset = dataset_filtered[train_dataset_size:]\n",
    "\n",
    "train_target = targets[:train_dataset_size]\n",
    "test_target = targets[train_dataset_size:]\n",
    "\n",
    "print(\"train_dataset_size = {}\\ntest_dataset_size = {}\".format(len(train_dataset), len(test_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_estimators=100, learning_rate=0.1, loss='lad' 0.82\n",
    "n_estimators=100, learning_rate=0.2, loss='lad' 0.84\n",
    "n_estimators=100, learning_rate=0.2, loss='huber' 0.84\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=1, gamma=0,\n",
       "       learning_rate=0.1, max_delta_step=0, max_depth=8,\n",
       "       min_child_weight=1, missing=None, n_estimators=400, nthread=-1,\n",
       "       objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=False, subsample=1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = xgb.XGBRegressor(max_depth=8, n_estimators=400, silent=False)\n",
    "print(\"Fitting model\")\n",
    "model.fit(train_dataset, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cv = StratifiedKFold(train_dataset, n_folds=5)\n",
    "print(\"Validating\")\n",
    "###scoring\n",
    "scores = cross_validation.cross_val_score(model, train_dataset, train_target, cv=5)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "#Evaluate the quality of the prediction\n",
    "test_predictions = model.predict(test_dataset)\n",
    "quality = sklearn.metrics.mean_absolute_error(test_predictions, test_target)\n",
    "print(\"Errors: %0.2f\" % quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting ...\n",
      "Loading dataset ...\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicting ...\")\n",
    "test_dataset = load_dataset(file=\"data/ech_test.csv\",\n",
    "                            columns=columns,\n",
    "                            value_filter=cleanup_data)\n",
    "\n",
    "test_dataset_filtered = filter_dataset(dataset=test_dataset,\n",
    "                                       features=feature_to_filter,\n",
    "                                       transform=float)\n",
    "\n",
    "with open(\"result.csv\", mode=\"w\") as outfile:\n",
    "    csv_writer = csv.writer(outfile, delimiter=\";\")\n",
    "    result = model.predict(test_dataset_filtered)\n",
    "    csv_writer.writerow((\"id\", \"prime_tot_ttc\"))\n",
    "    for res, row in zip(result, test_dataset):\n",
    "        csv_writer.writerow((row['id'], res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
